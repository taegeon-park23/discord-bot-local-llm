import json
from openai import OpenAI
from src.config import LLM_HOST, GEMINI_API_KEYS, GEMINI_MODEL
from src.logger import get_logger

logger = get_logger(__name__)

class AIAgent:
    def __init__(self):
        self.gemini_keys = GEMINI_API_KEYS
        self.local_url = LLM_HOST
        # Gemini OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
        self.gemini_base_url = "https://generativelanguage.googleapis.com/v1beta/openai/"
        
        logger.info(f"[AI] ì´ˆê¸°í™”: Gemini í‚¤ {len(self.gemini_keys)}ê°œ ê°ì§€, Local Fallback: {self.local_url}")

    def _get_client(self, is_local=False, api_key=None):
        """ìƒí™©ì— ë§ëŠ” OpenAI í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•˜ì—¬ ë°˜í™˜"""
        if is_local:
            return OpenAI(base_url=self.local_url, api_key="lm-studio")
        else:
            return OpenAI(base_url=self.gemini_base_url, api_key=api_key)

    def _call_llm_with_failover(self, messages, temperature=0.1):
        """
        [Failover ì „ëµ]
        1. Gemini Key ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì‹œë„
        2. ëª¨ë“  Gemini Key ì‹¤íŒ¨ ì‹œ -> Local LLM ì‹œë„
        3. Local LLM ì‹¤íŒ¨ ì‹œ -> None ë°˜í™˜
        """
        
        # 1. Gemini API ì‹œë„ (Key Rotation)
        for idx, key in enumerate(self.gemini_keys):
            try:
                client = self._get_client(is_local=False, api_key=key)
                logger.info(f"[AI] Gemini API ì‹œë„ (Key #{idx+1})")
                
                response = client.chat.completions.create(
                    model=GEMINI_MODEL,
                    messages=messages,
                    temperature=temperature
                )
                return response.choices[0].message.content
            except Exception as e:
                logger.warning(f"[AI] Gemini (Key #{idx+1}) ì‹¤íŒ¨: {e}")
                continue # ë‹¤ìŒ í‚¤ ì‹œë„

        # 2. Local LLM Fallback
        logger.warning("[AI] âš ï¸ ëª¨ë“  Gemini API ì‹¤íŒ¨. Local LLMìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
        try:
            client = self._get_client(is_local=True)
            response = client.chat.completions.create(
                model="local-model",
                messages=messages,
                temperature=temperature
            )
            logger.info("[AI] Local LLM ì‘ë‹µ ì„±ê³µ")
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"[AI] âŒ Local LLMë§ˆì € ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
            return None

    def analyze(self, text):
        if not text or len(text) < 50: return None
        
        system_prompt = """
You are a technical content summarizer.
Analyze the provided text and output ONLY valid JSON.
Format: {"title":"Korean Title","summary":"3 bullet points in Korean","category":"Tech/AI/Eco","tags":["tag1"],"difficulty":"Easy/Med/Hard"}
"""
        messages = [            
            {"role": "user", "content": f"{system_prompt}\n\n--- Input Text ---\n{text[:15000]}"}
        ]

        content = self._call_llm_with_failover(messages, temperature=0.1)
        
        if not content: return None

        try:
            # JSON íŒŒì‹± ë³´ì •
            clean_json = content.replace("```json", "").replace("```", "").strip()
            start, end = clean_json.find('{'), clean_json.rfind('}') + 1
            if start != -1 and end != -1: clean_json = clean_json[start:end]
            return json.loads(clean_json)
        except Exception as e:
            logger.error(f"[AI] JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None

    def deep_dive(self, text):
        if not text or len(text) < 50: return None
        
        system_prompt = """
You are a Senior Technical Researcher. 
Conduct a comprehensive Deep Dive analysis of the provided text.
Output MUST be in Korean Markdown format.
Structure:
# [Title]
## 1. ğŸ” í•µì‹¬ ë…¼ê±° ë° ì¸ì‚¬ì´íŠ¸
## 2. âš™ï¸ ê¸°ìˆ ì  ì‹¬ì¸µ ë¶„ì„
## 3. âš–ï¸ ë¹„íŒì  ì‹œê°
## 4. ğŸš€ ì‹¤ë¬´ ì ìš© í¬ì¸íŠ¸
"""
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": f"Analyze:\n{text[:30000]}"} # Gemini Long Context ëŒ€í­ í™œìš©
        ]

        return self._call_llm_with_failover(messages, temperature=0.3)

    def chat(self, messages, temperature=0.1):
        """Standard chat interface with failover support"""
        return self._call_llm_with_failover(messages, temperature)
